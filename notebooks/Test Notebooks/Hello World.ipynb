{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e418ba06-fcc4-40ac-8e94-71e468ec3dc1",
   "metadata": {},
   "source": [
    "# TUTORIAL: Tensorflow basic computation using single CPU or GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d204a50-193d-48dd-8d35-0abd42d48c12",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The aim of this tutorial is to use `AI TRAINING` product to do a __very simple tensor computation__ within the `Tensorflow` library and to compare the performance of running it over CPU versus GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1070f0b-901f-4c7b-a9d3-c9252272589d",
   "metadata": {},
   "source": [
    "## In practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a2f87-4005-442e-b31c-300a3d49ea39",
   "metadata": {},
   "source": [
    "### Step 1: Import Tensorflow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1cd9474-2412-438c-af40-3e8f504da374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-22 17:05:05.014939: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294426f5-312e-4df4-9b29-f44a39d7bd4b",
   "metadata": {},
   "source": [
    "### Step 2: Verify you have a GPU available on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74357c6-963e-45cd-9329-290d380c1a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-22 17:05:06.316126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 GPU device(s) have been found on your machine :\n",
      "* GPU n째0 whose name is \"/device:GPU:0\"\n",
      "1 CPU device(s) have been found on your machine :\n",
      "* CPU n째0 whose name is \"/device:CPU:0\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-22 17:05:06.320028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-22 17:05:06.320232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-22 17:05:06.323380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-22 17:05:06.323684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-22 17:05:06.323988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-22 17:05:06.842447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-22 17:05:06.842779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-22 17:05:06.842799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-07-22 17:05:06.843001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-22 17:05:06.843037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21597 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Get the list of all logical GPU device on your notebook\n",
    "GPU_DEVICES = tf.config.list_logical_devices('GPU')\n",
    "# Get the list of all logical CPU device on your notebook\n",
    "CPU_DEVICES = tf.config.list_logical_devices('CPU')\n",
    "# Keep only the names of each GPU devices\n",
    "GPU_DEVICES_NAMES = [x.name for x in GPU_DEVICES]\n",
    "# Keep only the names of each CPU devices\n",
    "CPU_DEVICES_NAMES = [x.name for x in CPU_DEVICES]\n",
    "# The number of GPU devices\n",
    "GPU_DEVICES_NB = len(GPU_DEVICES)\n",
    "# The number of CPU devices\n",
    "CPU_DEVICES_NB = len(CPU_DEVICES)\n",
    "\n",
    "\n",
    "if GPU_DEVICES_NB == 0:\n",
    "    raise SystemError('No GPU device found')\n",
    "else:\n",
    "    print(f'{GPU_DEVICES_NB} GPU device(s) have been found on your machine :')\n",
    "\n",
    "for nb in range(GPU_DEVICES_NB):\n",
    "    gpu_name = GPU_DEVICES_NAMES[nb]\n",
    "    print(f'* GPU n째{nb} whose name is \"{gpu_name}\"')\n",
    "\n",
    "\n",
    "if CPU_DEVICES_NB == 0:\n",
    "    raise SystemError('No CPU device found')\n",
    "else:\n",
    "    print(f'{CPU_DEVICES_NB} CPU device(s) have been found on your machine :')\n",
    "\n",
    "for nb in range(CPU_DEVICES_NB):\n",
    "    cpu_name = CPU_DEVICES_NAMES[nb]\n",
    "    print(f'* CPU n째{nb} whose name is \"{cpu_name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb604e-187c-453d-857c-1d7056e3216c",
   "metadata": {},
   "source": [
    "### Step 3: Define the operation to benchmark\n",
    "Here we choose to define a simple function that multiply 2 random vectors of the given length. This is the function that we are going to benchmark over available devices (GPU and CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938c2bb2-21e3-4ec1-9364-e4ed57034133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_multiply(vector_length):\n",
    "    vector_1 = tf.random.normal(vector_length)\n",
    "    vector_2 = tf.random.normal(vector_length)\n",
    "    return vector_1 * vector_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2e93a-3304-4b01-8dba-f2afc4cca413",
   "metadata": {},
   "source": [
    "### Step 4: Define the function executing the operation on the GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "167f0de1-9028-4906-bd4d-699bba8f45c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_operation(vector_length):\n",
    "    # If you have several GPU you can select the one to use by changing the used index of GPU_DEVICES_NAMES\n",
    "    with tf.device(GPU_DEVICES_NAMES[0]):\n",
    "        random_multiply(vector_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90278c9f-6e0d-46cf-b12e-2fc331db9520",
   "metadata": {},
   "source": [
    "### Step 5: Define the function executing on the CPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0a0f3c-8dc7-43b5-8c5d-475c21761c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_operation(vector_length):\n",
    "    # If you have several CPU you can select the one to use by changing the used index of GPU_DEVICES_NAMES\n",
    "    with tf.device(CPU_DEVICES_NAMES[0]):\n",
    "        random_multiply(vector_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87f383-39ed-456b-89e9-07096a375dde",
   "metadata": {},
   "source": [
    "### Step6 : Launch the benchmark of each device over several vectors of different lengths\n",
    "Here we are going to iterate over several lengths of vectors and launch a benchmark both on GPU and CPU to observe on which cases GPU is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed994a08-6db9-41b8-9d2c-fb21799c4036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations on vector of length 1 are 0.23175491470848275x faster on GPU than CPU\n",
      "Operations on vector of length 10 are 0.03949014161861367x faster on GPU than CPU\n",
      "Operations on vector of length 100 are 0.3369371306840197x faster on GPU than CPU\n",
      "Operations on vector of length 1000 are 0.4360783151711389x faster on GPU than CPU\n",
      "Operations on vector of length 10000 are 1.2211463012744876x faster on GPU than CPU\n",
      "Operations on vector of length 100000 are 3.917722085341833x faster on GPU than CPU\n",
      "Operations on vector of length 1000000 are 15.130434526520064x faster on GPU than CPU\n",
      "Operations on vector of length 10000000 are 119.62252363105078x faster on GPU than CPU\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
    "cpu_operation([1])\n",
    "gpu_operation([1])\n",
    "\n",
    "for i in range(8):\n",
    "    vector_length = pow(10, i)\n",
    "    cpu_time = timeit.timeit(f'cpu_operation([{vector_length}])', number=20, setup=\"from __main__ import cpu_operation\")\n",
    "    gpu_time = timeit.timeit(f'gpu_operation([{vector_length}])', number=20, setup=\"from __main__ import gpu_operation\")\n",
    "    print(f'Operations on vector of length {vector_length} are {cpu_time/gpu_time}x faster on GPU than CPU')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
